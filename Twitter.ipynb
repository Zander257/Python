{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import re as regularExpression\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts ticker symbols from a given tweet text. \n",
    "# It uses a regular expression pattern to match ticker symbols that start with a dollar sign ($) \n",
    "# followed by one or more alphabetical characters (A-Z, a-z) with a length of 3 to 4 characters\n",
    "def getTickers(tweet_text):\n",
    "    pattern = r\"\\$([A-Za-z]{3,4}+)\"\n",
    "    tickers = regularExpression.findall(pattern, tweet_text)\n",
    "    return tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the web page cannot be read from or to get the stock symbols, it works on a normal text format though\n",
    "\n",
    "# responsible for scraping a specific Twitter account's page, extracting the tweet text, \n",
    "# and counting the occurrences of a given ticker symbol within the tweet text.\n",
    "def scrapeAccount(url, ticker, interval):\n",
    "    chrome_driver = webdriver.Chrome(service=ChromeService())\n",
    "    chrome_driver.get(url)\n",
    "    soup = bs(chrome_driver.page_source, 'html.parser')\n",
    "\n",
    "    tweets = soup.find_all('div', class_='css-175oi2r r-18u37iz r-1udh08x r-1c4vpko r-1c7gwzm r-o7ynqc r-6416eg r-1ny4l3l r-1loqt21')\n",
    "    ticker_count = 0\n",
    "\n",
    "    for tweet in tweets:\n",
    "        tweet_text = tweet.find('div', class_='css-1jxf684 r-bcqeeo r-1ttztb7 r-qvutc0 r-poiln3 r-1ny4l3l r-1ddef8g r-tjvw6i r-1loqt21')\n",
    "        if tweet_text is not None:\n",
    "            tweet_text = tweet_text.text.strip()\n",
    "            tickers = getTickers(tweet_text)\n",
    "\n",
    "            if ticker in tickers:\n",
    "                ticker_count += 1\n",
    "\n",
    "    print(f\"{ticker} was mentioned {ticker_count} times in the last {interval} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responsible for continuously scraping the specified Twitter accounts for a given ticker symbol at regular intervals\n",
    "def scrapeThroughAccounts(accounts, ticker, interval):\n",
    "    while True:\n",
    "        for account in accounts:\n",
    "            url = f\"{account}\"\n",
    "            scrapeAccount(url, ticker, interval)\n",
    "        time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts = [\n",
    "    \"https://twitter.com/Mr_Derivatives\",\n",
    "    \"https://twitter.com/warrior_0719\",\n",
    "    \"https://twitter.com/ChartingProdigy\",\n",
    "    \"https://twitter.com/allstarcharts\",\n",
    "    \"https://twitter.com/yuriymatso\",\n",
    "    \"https://twitter.com/TriggerTrades\",\n",
    "    \"https://twitter.com/AdamMancini4\",\n",
    "    \"https://twitter.com/CordovaTrades\",\n",
    "    \"https://twitter.com/Barchart\",\n",
    "    \"https://twitter.com/RoyLMattox\"\n",
    "]\n",
    "\n",
    "ticker = \"$TSLA\"\n",
    "time_interval = 15  # in minutes\n",
    "\n",
    "scrapeThroughAccounts(accounts, ticker, time_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is just an example that the code works properly and I understand the concept needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tickers_list = []\n",
    "ticker_count = 0\n",
    "\n",
    "def getFileTickers(text):\n",
    "    pattern = r\"\\$([A-Za-z]{3,4}+)\"\n",
    "    tickers = regularExpression.findall(pattern, text)\n",
    "    return tickers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def appendTickers(file_paths):\n",
    "    ticker_count = 0\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "        tickers = getFileTickers(text)\n",
    "        ticker_count += tickers.count(ticker)\n",
    "print(Tickers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeFile(file_paths, Tickers_list, interval):\n",
    "    ticker_count = 0\n",
    "    for file_path in file_paths:\n",
    "        tickers = getFileTickers(file_path)\n",
    "        if ticker in Tickers_list:\n",
    "            ticker_count += 1\n",
    "\n",
    "    print(f\"{ticker} was mentioned {ticker_count} times in the last {interval} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$TSLA was mentioned 10 times in the last 30 minutes.\n"
     ]
    }
   ],
   "source": [
    "file_paths = [\n",
    "    \"C:/Users/zainm/OneDrive/Desktop/Twitter Scraper/Mr_Derivatives.txt\", \n",
    "    \"C:/Users/zainm/OneDrive/Desktop/Twitter Scraper/warrior_0719.txt\",  \n",
    "    \"C:/Users/zainm/OneDrive/Desktop/Twitter Scraper/ChartingProdigy.txt\",  \n",
    "    \"C:/Users/zainm/OneDrive/Desktop/Twitter Scraper/allstarcharts.txt\",  \n",
    "    \"C:/Users/zainm/OneDrive/Desktop/Twitter Scraper/yuriymatso.txt\",  \n",
    "    \"C:/Users/zainm/OneDrive/Desktop/Twitter Scraper/TriggerTrades.txt\", \n",
    "    \"C:/Users/zainm/OneDrive/Desktop/Twitter Scraper/AdamMancini4.txt\",  \n",
    "    \"C:/Users/zainm/OneDrive/Desktop/Twitter Scraper/CordovaTrades.txt\", \n",
    "    \"C:/Users/zainm/OneDrive/Desktop/Twitter Scraper/Barchart.txt\",  \n",
    "    \"C:/Users/zainm/OneDrive/Desktop/Twitter Scraper/RoyLMattox.txt\",  \n",
    "]\n",
    "\n",
    "ticker = \"$TSLA\"\n",
    "interval = 30\n",
    "\n",
    "scrapeFile(file_paths, ticker, interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
